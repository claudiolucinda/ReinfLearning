{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNQo1NWewjm15cJUqfpD3Zi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiolucinda/ReinfLearning/blob/master/PS05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QCneAGZcqyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "def plot_learning_curve(x, scores, epsilons, lines=None):\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "\n",
        "class RepeatActionAndMaxFrame(gym.Wrapper):\n",
        "    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,\n",
        "                 fire_first=False):\n",
        "        super(RepeatActionAndMaxFrame, self).__init__(env)\n",
        "        self.repeat = repeat\n",
        "        self.shape = env.observation_space.low.shape\n",
        "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
        "        self.clip_reward = clip_reward\n",
        "        self.no_ops = no_ops\n",
        "        self.fire_first = fire_first\n",
        "\n",
        "    def step(self, action):\n",
        "        t_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self.repeat):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if self.clip_reward:\n",
        "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
        "            t_reward += reward\n",
        "            idx = i % 2\n",
        "            self.frame_buffer[idx] = obs\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
        "        return max_frame, t_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops > 0 else 0\n",
        "        for _ in range(no_ops):\n",
        "            _, _, done, _ = self.env.step(0)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "        if self.fire_first:\n",
        "            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "            obs, _, _, _ = self.env.step(1)\n",
        "\n",
        "        self.frame_buffer = np.zeros_like((2,self.shape))\n",
        "        self.frame_buffer[0] = obs\n",
        "\n",
        "        return obs\n",
        "\n",
        "class PreprocessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, shape, env=None):\n",
        "        super(PreprocessFrame, self).__init__(env)\n",
        "        self.shape = (shape[2], shape[0], shape[1])\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
        "                                    shape=self.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        resized_screen = cv2.resize(new_frame, self.shape[1:],\n",
        "                                    interpolation=cv2.INTER_AREA)\n",
        "        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
        "        new_obs = new_obs / 255.0\n",
        "\n",
        "        return new_obs\n",
        "\n",
        "class StackFrames(gym.ObservationWrapper):\n",
        "    def __init__(self, env, repeat):\n",
        "        super(StackFrames, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "                            env.observation_space.low.repeat(repeat, axis=0),\n",
        "                            env.observation_space.high.repeat(repeat, axis=0),\n",
        "                            dtype=np.float32)\n",
        "        self.stack = collections.deque(maxlen=repeat)\n",
        "\n",
        "    def reset(self):\n",
        "        self.stack.clear()\n",
        "        observation = self.env.reset()\n",
        "        for _ in range(self.stack.maxlen):\n",
        "            self.stack.append(observation)\n",
        "\n",
        "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.stack.append(observation)\n",
        "\n",
        "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "#Clip Rewards = limitar rewards entre -1 e 1\n",
        "\n",
        "def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,\n",
        "             no_ops=0, fire_first=False):\n",
        "    env = gym.make(env_name)\n",
        "    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)\n",
        "    env = PreprocessFrame(shape, env)\n",
        "    env = StackFrames(env, repeat)\n",
        "\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYOGHleTuDbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                     dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                         dtype=np.float32)\n",
        "\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlECvRbzvI6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
        "\n",
        "        fc_input_dims = self.calculate_conv_output_dims(input_dims)\n",
        "\n",
        "        self.fc1 = nn.Linear(fc_input_dims, 512)\n",
        "        self.fc2 = nn.Linear(512, n_actions)\n",
        "\n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
        "\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def calculate_conv_output_dims(self, input_dims):\n",
        "        state = T.zeros(1, *input_dims)\n",
        "        dims = self.conv1(state)\n",
        "        dims = self.conv2(dims)\n",
        "        dims = self.conv3(dims)\n",
        "        return int(np.prod(dims.size()))\n",
        "\n",
        "    def forward(self, state):\n",
        "        conv1 = F.relu(self.conv1(state))\n",
        "        conv2 = F.relu(self.conv2(conv1))\n",
        "        conv3 = F.relu(self.conv3(conv2))\n",
        "        # conv3 shape is BS x n_filters x H x W\n",
        "        conv_state = conv3.view(conv3.size()[0], -1)\n",
        "        # conv_state shape is BS x (n_filters * H * W)\n",
        "        flat1 = F.relu(self.fc1(conv_state))\n",
        "        actions = self.fc2(flat1)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPs2AlO7yV4J",
        "colab_type": "code",
        "outputId": "d3a1679d-fc46-404a-fce8-23717fa6a474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pwd\n",
        "!mkdir reinf\n",
        "% cd reinf\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/reinf\n",
            "/content/reinf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J4k0hsQvQqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch as T\n",
        "\n",
        "class DQNAgent(object):\n",
        "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
        "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
        "                 replace=1000, algo=None, env_name=None, chkpt_dir='content/reinf'):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.lr = lr\n",
        "        self.n_actions = n_actions\n",
        "        self.input_dims = input_dims\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_min = eps_min\n",
        "        self.eps_dec = eps_dec\n",
        "        self.replace_target_cnt = replace\n",
        "        self.algo = algo\n",
        "        self.env_name = env_name\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
        "\n",
        "        self.q_eval = DeepQNetwork(self.lr, self.n_actions,\n",
        "                                    input_dims=self.input_dims,\n",
        "                                    name=self.env_name+'_'+self.algo+'_q_eval',\n",
        "                                    chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "        self.q_next = DeepQNetwork(self.lr, self.n_actions,\n",
        "                                    input_dims=self.input_dims,\n",
        "                                    name=self.env_name+'_'+self.algo+'_q_next',\n",
        "                                    chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
        "            actions = self.q_eval.forward(state)\n",
        "            action = T.argmax(actions).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        self.memory.store_transition(state, action, reward, state_, done)\n",
        "\n",
        "    def sample_memory(self):\n",
        "        state, action, reward, new_state, done = \\\n",
        "                                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        states = T.tensor(state).to(self.q_eval.device)\n",
        "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
        "        dones = T.tensor(done).to(self.q_eval.device)\n",
        "        actions = T.tensor(action).to(self.q_eval.device)\n",
        "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
        "\n",
        "        return states, actions, rewards, states_, dones\n",
        "\n",
        "    def replace_target_network(self):\n",
        "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
        "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
        "\n",
        "    def decrement_epsilon(self):\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                           if self.epsilon > self.eps_min else self.eps_min\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_eval.save_checkpoint()\n",
        "        self.q_next.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_eval.load_checkpoint()\n",
        "        self.q_next.load_checkpoint()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.q_eval.optimizer.zero_grad()\n",
        "\n",
        "        self.replace_target_network()\n",
        "\n",
        "        states, actions, rewards, states_, dones = self.sample_memory()\n",
        "        indices = np.arange(self.batch_size)\n",
        "\n",
        "        q_pred = self.q_eval.forward(states)[indices, actions]\n",
        "        q_next = self.q_next.forward(states_).max(dim=1)[0]\n",
        "\n",
        "        q_next[dones] = 0.0\n",
        "        q_target = rewards + self.gamma*q_next\n",
        "\n",
        "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
        "        loss.backward()\n",
        "        self.q_eval.optimizer.step()\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "        self.decrement_epsilon()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA1939O5GP-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f8a3633-ec9e-4623-98c4-747a887d4710"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "#from dqn_agent import DQNAgent\n",
        "#from utils import plot_learning_curve, make_env\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = make_env('PongNoFrameskip-v4')\n",
        "    best_score = -np.inf\n",
        "    load_checkpoint = False\n",
        "    n_games = 100\n",
        "    agent = DQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,\n",
        "                     input_dims=(env.observation_space.shape),\n",
        "                     n_actions=env.action_space.n, mem_size=50000, eps_min=0.1,\n",
        "                     batch_size=32, replace=1000, eps_dec=1e-5,\n",
        "                     chkpt_dir='content/reinf', algo='DQNAgent',\n",
        "                     env_name='PongNoFrameskip-v4')\n",
        "\n",
        "    if load_checkpoint:\n",
        "        agent.load_models()\n",
        "\n",
        "    fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_' \\\n",
        "            + str(n_games) + 'games'\n",
        "    figure_file = 'plots/' + fname + '.png'\n",
        "\n",
        "    n_steps = 0\n",
        "    scores, eps_history, steps_array = [], [], []\n",
        "\n",
        "    for i in range(n_games):\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "\n",
        "        score = 0\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "\n",
        "            if not load_checkpoint:\n",
        "                agent.store_transition(observation, action,\n",
        "                                     reward, observation_, int(done))\n",
        "                agent.learn()\n",
        "            observation = observation_\n",
        "            n_steps += 1\n",
        "        scores.append(score)\n",
        "        steps_array.append(n_steps)\n",
        "\n",
        "        avg_score = np.mean(scores[-100:])\n",
        "        print('episode: ', i,'score: ', score,\n",
        "             ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n",
        "            'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
        "\n",
        "        if avg_score > best_score:\n",
        "            #if not load_checkpoint:\n",
        "            #    agent.save_models()\n",
        "            best_score = avg_score\n",
        "\n",
        "        eps_history.append(agent.epsilon)\n",
        "        if load_checkpoint and n_steps >= 18000:\n",
        "            break\n",
        "\n",
        "    x = [i+1 for i in range(len(scores))]\n",
        "    plot_learning_curve(steps_array, scores, eps_history)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode:  0 score:  -21.0  average score -21.0 best score -inf epsilon 0.99 steps 912\n",
            "episode:  1 score:  -21.0  average score -21.0 best score -21.00 epsilon 0.98 steps 1796\n",
            "episode:  2 score:  -19.0  average score -20.3 best score -21.00 epsilon 0.97 steps 2751\n",
            "episode:  3 score:  -20.0  average score -20.2 best score -20.33 epsilon 0.96 steps 3850\n",
            "episode:  4 score:  -21.0  average score -20.4 best score -20.25 epsilon 0.95 steps 4705\n",
            "episode:  5 score:  -19.0  average score -20.2 best score -20.25 epsilon 0.94 steps 5621\n",
            "episode:  6 score:  -19.0  average score -20.0 best score -20.17 epsilon 0.93 steps 6555\n",
            "episode:  7 score:  -21.0  average score -20.1 best score -20.00 epsilon 0.93 steps 7379\n",
            "episode:  8 score:  -21.0  average score -20.2 best score -20.00 epsilon 0.92 steps 8323\n",
            "episode:  9 score:  -21.0  average score -20.3 best score -20.00 epsilon 0.91 steps 9127\n",
            "episode:  10 score:  -18.0  average score -20.1 best score -20.00 epsilon 0.90 steps 10388\n",
            "episode:  11 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.89 steps 11352\n",
            "episode:  12 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.88 steps 12343\n",
            "episode:  13 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.87 steps 13341\n",
            "episode:  14 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.86 steps 14425\n",
            "episode:  15 score:  -21.0  average score -20.1 best score -20.00 epsilon 0.85 steps 15236\n",
            "episode:  16 score:  -19.0  average score -20.1 best score -20.00 epsilon 0.84 steps 16270\n",
            "episode:  17 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.83 steps 17370\n",
            "episode:  18 score:  -21.0  average score -20.1 best score -20.00 epsilon 0.82 steps 18222\n",
            "episode:  19 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.81 steps 19199\n",
            "episode:  20 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.80 steps 20270\n",
            "episode:  21 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.79 steps 21168\n",
            "episode:  22 score:  -21.0  average score -20.1 best score -20.00 epsilon 0.78 steps 22114\n",
            "episode:  23 score:  -19.0  average score -20.1 best score -20.00 epsilon 0.77 steps 23114\n",
            "episode:  24 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.76 steps 23979\n",
            "episode:  25 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.75 steps 25057\n",
            "episode:  26 score:  -21.0  average score -20.1 best score -20.00 epsilon 0.74 steps 26155\n",
            "episode:  27 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.73 steps 27366\n",
            "episode:  28 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.72 steps 28529\n",
            "episode:  29 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.70 steps 29566\n",
            "episode:  30 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.69 steps 30707\n",
            "episode:  31 score:  -20.0  average score -20.1 best score -20.00 epsilon 0.68 steps 31930\n",
            "episode:  32 score:  -19.0  average score -20.1 best score -20.00 epsilon 0.67 steps 33112\n",
            "episode:  33 score:  -19.0  average score -20.0 best score -20.00 epsilon 0.66 steps 34498\n",
            "episode:  34 score:  -21.0  average score -20.1 best score -20.00 epsilon 0.64 steps 35533\n",
            "episode:  35 score:  -19.0  average score -20.0 best score -20.00 epsilon 0.63 steps 36572\n",
            "episode:  36 score:  -19.0  average score -20.0 best score -20.00 epsilon 0.62 steps 37766\n",
            "episode:  37 score:  -17.0  average score -19.9 best score -20.00 epsilon 0.61 steps 39282\n",
            "episode:  38 score:  -19.0  average score -19.9 best score -19.92 epsilon 0.60 steps 40494\n",
            "episode:  39 score:  -19.0  average score -19.9 best score -19.90 epsilon 0.58 steps 42007\n",
            "episode:  40 score:  -20.0  average score -19.9 best score -19.88 epsilon 0.57 steps 43288\n",
            "episode:  41 score:  -16.0  average score -19.8 best score -19.88 epsilon 0.55 steps 45093\n",
            "episode:  42 score:  -20.0  average score -19.8 best score -19.79 epsilon 0.53 steps 46644\n",
            "episode:  43 score:  -17.0  average score -19.7 best score -19.79 epsilon 0.52 steps 48269\n",
            "episode:  44 score:  -18.0  average score -19.7 best score -19.73 epsilon 0.51 steps 49489\n",
            "episode:  45 score:  -14.0  average score -19.6 best score -19.69 epsilon 0.49 steps 51261\n",
            "episode:  46 score:  -17.0  average score -19.5 best score -19.57 epsilon 0.47 steps 53048\n",
            "episode:  47 score:  -15.0  average score -19.4 best score -19.51 epsilon 0.45 steps 54836\n",
            "episode:  48 score:  -18.0  average score -19.4 best score -19.42 epsilon 0.43 steps 56549\n",
            "episode:  49 score:  -18.0  average score -19.4 best score -19.39 epsilon 0.42 steps 58241\n",
            "episode:  50 score:  -16.0  average score -19.3 best score -19.36 epsilon 0.40 steps 60158\n",
            "episode:  51 score:  -17.0  average score -19.2 best score -19.29 epsilon 0.38 steps 62230\n",
            "episode:  52 score:  -18.0  average score -19.2 best score -19.25 epsilon 0.36 steps 63846\n",
            "episode:  53 score:  -14.0  average score -19.1 best score -19.23 epsilon 0.34 steps 65735\n",
            "episode:  54 score:  -17.0  average score -19.1 best score -19.13 epsilon 0.33 steps 67486\n",
            "episode:  55 score:  -15.0  average score -19.0 best score -19.09 epsilon 0.31 steps 69442\n",
            "episode:  56 score:  -17.0  average score -19.0 best score -19.02 epsilon 0.29 steps 71111\n",
            "episode:  57 score:  -16.0  average score -18.9 best score -18.98 epsilon 0.27 steps 73031\n",
            "episode:  58 score:  -16.0  average score -18.9 best score -18.93 epsilon 0.25 steps 75073\n",
            "episode:  59 score:  -16.0  average score -18.8 best score -18.88 epsilon 0.23 steps 77103\n",
            "episode:  60 score:  -14.0  average score -18.8 best score -18.83 epsilon 0.21 steps 79289\n",
            "episode:  61 score:  -13.0  average score -18.7 best score -18.75 epsilon 0.18 steps 81979\n",
            "episode:  62 score:  -13.0  average score -18.6 best score -18.66 epsilon 0.16 steps 84463\n",
            "episode:  63 score:  -13.0  average score -18.5 best score -18.57 epsilon 0.13 steps 86737\n",
            "episode:  64 score:  -14.0  average score -18.4 best score -18.48 epsilon 0.11 steps 89319\n",
            "episode:  65 score:  -15.0  average score -18.4 best score -18.42 epsilon 0.10 steps 91842\n",
            "episode:  66 score:  -10.0  average score -18.2 best score -18.36 epsilon 0.10 steps 95492\n",
            "episode:  67 score:  -17.0  average score -18.2 best score -18.24 epsilon 0.10 steps 97608\n",
            "episode:  68 score:  -11.0  average score -18.1 best score -18.22 epsilon 0.10 steps 100686\n",
            "episode:  69 score:  -5.0  average score -17.9 best score -18.12 epsilon 0.10 steps 103972\n",
            "episode:  70 score:  -20.0  average score -18.0 best score -17.93 epsilon 0.10 steps 106566\n",
            "episode:  71 score:  -17.0  average score -17.9 best score -17.93 epsilon 0.10 steps 108956\n",
            "episode:  72 score:  -11.0  average score -17.8 best score -17.93 epsilon 0.10 steps 112399\n",
            "episode:  73 score:  -9.0  average score -17.7 best score -17.85 epsilon 0.10 steps 115987\n",
            "episode:  74 score:  -18.0  average score -17.7 best score -17.73 epsilon 0.10 steps 118325\n",
            "episode:  75 score:  -8.0  average score -17.6 best score -17.73 epsilon 0.10 steps 122324\n",
            "episode:  76 score:  -13.0  average score -17.5 best score -17.61 epsilon 0.10 steps 125080\n",
            "episode:  77 score:  -17.0  average score -17.5 best score -17.55 epsilon 0.10 steps 127602\n",
            "episode:  78 score:  -16.0  average score -17.5 best score -17.54 epsilon 0.10 steps 130699\n",
            "episode:  79 score:  -5.0  average score -17.4 best score -17.52 epsilon 0.10 steps 134546\n",
            "episode:  80 score:  -7.0  average score -17.2 best score -17.36 epsilon 0.10 steps 137890\n",
            "episode:  81 score:  -7.0  average score -17.1 best score -17.23 epsilon 0.10 steps 141691\n",
            "episode:  82 score:  -16.0  average score -17.1 best score -17.11 epsilon 0.10 steps 143927\n",
            "episode:  83 score:  -8.0  average score -17.0 best score -17.10 epsilon 0.10 steps 146864\n",
            "episode:  84 score:  -15.0  average score -17.0 best score -16.99 epsilon 0.10 steps 149458\n",
            "episode:  85 score:  -6.0  average score -16.8 best score -16.96 epsilon 0.10 steps 153150\n",
            "episode:  86 score:  -3.0  average score -16.7 best score -16.84 epsilon 0.10 steps 156629\n",
            "episode:  87 score:  -15.0  average score -16.7 best score -16.68 epsilon 0.10 steps 159348\n",
            "episode:  88 score:  -10.0  average score -16.6 best score -16.66 epsilon 0.10 steps 162174\n",
            "episode:  89 score:  -3.0  average score -16.4 best score -16.58 epsilon 0.10 steps 165692\n",
            "episode:  90 score:  -9.0  average score -16.4 best score -16.43 epsilon 0.10 steps 168282\n",
            "episode:  91 score:  -2.0  average score -16.2 best score -16.35 epsilon 0.10 steps 172440\n",
            "episode:  92 score:  -2.0  average score -16.0 best score -16.20 epsilon 0.10 steps 176246\n",
            "episode:  93 score:  4.0  average score -15.8 best score -16.04 epsilon 0.10 steps 179644\n",
            "episode:  94 score:  -6.0  average score -15.7 best score -15.83 epsilon 0.10 steps 183255\n",
            "episode:  95 score:  -4.0  average score -15.6 best score -15.73 epsilon 0.10 steps 186843\n",
            "episode:  96 score:  1.0  average score -15.4 best score -15.60 epsilon 0.10 steps 191003\n",
            "episode:  97 score:  -12.0  average score -15.4 best score -15.43 epsilon 0.10 steps 193533\n",
            "episode:  98 score:  7.0  average score -15.2 best score -15.40 epsilon 0.10 steps 196844\n",
            "episode:  99 score:  7.0  average score -14.9 best score -15.17 epsilon 0.10 steps 199846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAEGCAYAAAA5T6EkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}